# Tank World Roadmap

> **Vision**: A framework for AI-driven automated Artificial Life research, where entertaining visualizations drive real scientific discovery through two-layer evolution.

Tank World will eventually be capable of conducting advanced Alife research autonomously while visualizing results in ways that are genuinely entertaining. The fish tank is the starting point—a proof of concept that AI can evolve algorithms through data-driven analysis. The end goal is a self-improving research framework that scales through distributed compute.

See [docs/VISION.md](docs/VISION.md) for the full project vision and [docs/PROJECT_PHILOSOPHY.md](docs/PROJECT_PHILOSOPHY.md) for core principles.

---

## Phase 1: Foundation (Current)

**Goal**: Prove the two-layer evolution concept works.

### Completed
- [x] 58 parametrizable behavior algorithms across 6 categories
- [x] Layer 1 evolution: population-level natural selection
- [x] Fish tank visualization with fractal plants and predators
- [x] Headless mode for fast data collection (10-300x speedup)
- [x] AI code evolution agent (Layer 2 proof of concept)
- [x] Proven improvement: 0% → 100% reproduction rate via AI
- [x] Algorithm registry with source file mapping
- [x] LLM-friendly JSON stats export
- [x] Deterministic seeding for reproducibility
- [x] Poker minigame for energy redistribution

### In Progress
- [ ] Improve algorithm genome structure for better inheritance
- [ ] Add clear metrics for algorithm performance comparison
- [ ] Solidify mutation and selection logic

---

## Phase 2: Closed Loop

**Goal**: Fully automate the evolution cycle so it runs continuously without human intervention.

### Milestones
- [ ] **Automated validation**: Run test simulations before accepting AI changes
- [ ] **Regression detection**: Reject changes that reduce fitness metrics
- [ ] **Batch improvements**: AI improves multiple algorithms per cycle
- [ ] **Scheduled runs**: Nightly evolution cycles via CI/CD
- [ ] **Self-healing**: AI fixes broken code it generates
- [ ] **Metrics dashboard**: Track improvement velocity over time

### Success Criteria
- Evolution runs for 1 week without human intervention
- Fitness metrics measurably improve each cycle
- No regressions make it to main branch

---

## Phase 3: Research Platform

**Goal**: Make Tank World a legitimate platform for Alife research.

### Milestones
- [ ] **Experiment framework**: Configure and run named experiments
- [ ] **Complexity metrics**: Measure behavioral complexity, novelty, and diversity
- [ ] **Open-endedness detection**: Identify when evolution is genuinely open-ended
- [ ] **Cross-experiment analysis**: Compare results across parameter variations
- [ ] **Data export for publication**: Generate research-ready datasets
- [ ] **Reproducibility tools**: Exact replay of evolutionary history
- [ ] **Multiple task domains**: Beyond poker to other algorithmic challenges

### Success Criteria
- First Alife research paper using Tank World data
- Measurable open-ended evolution over extended runs
- Community contributions to algorithm library

---

## Phase 4: Distributed Compute

**Goal**: Users contribute compute by running entertaining simulations.

### Milestones
- [ ] **Multi-tank network**: Interconnected tanks sharing discoveries
- [ ] **Algorithm migration**: Successful algorithms spread across tanks
- [ ] **User engagement metrics**: Track what makes simulations entertaining
- [ ] **Compute contribution tracking**: Fair credit for participation
- [ ] **Browser-based client**: No installation required
- [ ] **Mobile visualization**: Watch evolution on any device

### Success Criteria
- 100+ simultaneous tanks running worldwide
- Algorithm improvements discovered across network
- Users run simulations because they're fun, not obligation

---

## Phase 5: Evolving Visualization

**Goal**: AI evolves how research is presented—optimizing for engagement.

### Milestones
- [ ] **Visualization abstraction**: Separate rendering from simulation logic
- [ ] **AI-proposed entities**: New entity types generated by AI
- [ ] **Engagement metrics**: Track watch time, return visits, shares
- [ ] **A/B testing**: Compare visualization variants for engagement
- [ ] **Alternative metaphors**: Beyond fish tanks to other visual systems
- [ ] **User preference learning**: Adapt visuals to individual preferences

### Success Criteria
- AI successfully proposes and implements new entity types
- Visualization changes measurably improve engagement metrics
- Multiple visualization "species" coexist and compete
- Users watch simulations longer without human-designed improvements

---

## Phase 6: Self-Improving Framework

**Goal**: The framework expands its own capabilities without human design.

### Milestones
- [ ] **Meta-evolution**: AI improves the evolution system itself
- [ ] **Research question discovery**: AI proposes experiments to run next
- [ ] **Framework refactoring**: AI restructures codebase for new capabilities
- [ ] **Capability expansion**: System gains abilities humans didn't design
- [ ] **Cross-domain transfer**: Apply learned improvements to new problem domains

### Success Criteria
- AI proposes architectural changes that improve research output
- System complexity grows through AI evolution (not human commits)
- Emergent capabilities surprise developers
- Framework conducts meaningful research humans didn't specify

---

## Guiding Principles

1. **Interpretability**: Explicit algorithms over black-box neural networks—we can read and understand what evolved
2. **Human oversight (for now)**: AI proposes, humans approve—until the system proves trustworthy
3. **Data-driven**: All improvements grounded in simulation evidence, not speculation
4. **Entertainment value**: Visualization isn't optional—it's how we scale compute
5. **Open evolution**: Everything happens in the open, in git, with full history

---

## Current Focus

**Immediate priorities (Phase 1 → Phase 2 transition):**
1. Close the automated loop—AI runs without human intervention
2. Add validation simulations before accepting AI changes
3. Implement regression detection to reject harmful changes
4. Track and document successful AI improvements as proof
5. Improve algorithm diversity metrics

**Key questions we're exploring:**
- How do we measure "open-ended evolution" in practice?
- What fitness metrics actually predict long-term success?
- How much human oversight is needed before we can remove it?
- What makes a simulation entertaining enough to drive compute contributions?

---

*Last updated: December 2025*
